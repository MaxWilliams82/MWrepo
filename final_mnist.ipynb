{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importer les données\n",
    "(x_train_total, y_train_total), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train_total = x_train_total.reshape(60000, 784)/255\n",
    "x_test = x_test.reshape(10000, 784)/255\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_total, y_train_total, test_size=0.1, shuffle=True\n",
    ")\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# y_train_oh = np.array.zeros(size=(60000, 10))\n",
    "# for i in range(y_train.shape[0]):\n",
    "#   y_train_oh[i][y_train[i]] = 1\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train_OH = np.eye(10)[y_train]\n",
    "y_test_OH = np.eye(10)[y_test]\n",
    "y_val_OH = np.eye(10)[y_val]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#définir le nombre de neurones dans la couche cachée et output\n",
    "nb_neurons_h = 500\n",
    "nb_neurons_o = 10\n",
    "\n",
    "#générer des valeurs aléatoires pour chaque poids de biais\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "weights_h = -1 + 2*rng.random(size=(nb_neurons_h, 784))\n",
    "w_o = -1 + 2*rng.random(size=(nb_neurons_o, nb_neurons_h))\n",
    "\n",
    "\n",
    "bias_h = -1 + 2*rng.random(size=(nb_neurons_h))\n",
    "bias_o = -1 + 2*rng.random(size=(nb_neurons_o))\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(y):\n",
    "  return 1 / (1 + math.e**(-y))\n",
    "\n",
    "def deriveesigmoid(y):\n",
    "  return math.e**(-y) / (1 + math.e**(-y))**2\n",
    "\n",
    "def softmax(y):\n",
    "  exp_y = np.exp(y - np.max(y))\n",
    "  softmax = exp_y / np.sum(exp_y)\n",
    "  return softmax\n",
    "\n",
    "#categorical cross entropy loss\n",
    "def categorical_cross_entropy(target, z_o):\n",
    "  return np.sum(-np.log(z_o + 10**-100)*target)\n",
    "\n",
    "\n",
    "\n",
    "def forwardprop(x, weights_h, bias_h, w_o, bias_o):\n",
    "  y_h = (x @ weights_h.T) + (bias_h)\n",
    "  z_h = sigmoid(y_h)\n",
    "  y_o = (z_h @ w_o.T) + (bias_o)\n",
    "  z_o = softmax(y_o)\n",
    "  return y_h, z_h, y_o, z_o\n",
    "\n",
    "def backprop(y_h, z_h, y_o, z_o, weights_h, w_o, x, target):\n",
    "  d_y_o = (z_o - target)\n",
    "  d_bias_o = d_y_o.squeeze()\n",
    "  d_w_o = d_y_o.T @ z_h\n",
    "  d_y_h = (d_y_o @ w_o)*deriveesigmoid(y_h)\n",
    "  d_bias_h = d_y_h.squeeze()\n",
    "  d_weights_h = d_y_h.T @ x\n",
    "\n",
    "  return d_weights_h, d_bias_h, d_w_o, d_bias_o\n",
    "\n",
    "def update(weights_h, bias_h, w_o, bias_o, d_weights_h, d_bias_h, d_w_o, d_bias_o):\n",
    "  l_r = 0.01\n",
    "  bias_h -= l_r*d_bias_h\n",
    "  bias_o -= l_r*d_bias_o\n",
    "  weights_h -= l_r*d_weights_h\n",
    "  w_o -= l_r*d_w_o\n",
    "  return weights_h, bias_h, w_o, bias_o\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(weights_h, bias_h, w_o, bias_o, x, target):\n",
    "  nb_it = 5\n",
    "  loss_list = []\n",
    "  for i in trange(nb_it):\n",
    "    loss = 0\n",
    "\n",
    "    d_bias_h = np.zeros_like(bias_h)\n",
    "    d_bias_o = np.zeros_like(bias_o)\n",
    "\n",
    "    d_weights_h = np.zeros_like(weights_h)\n",
    "    d_w_o = np.zeros_like(w_o)\n",
    "\n",
    "    sample_indices = np.arange(x_train.shape[0])\n",
    "    rng.shuffle(sample_indices)    #bagging\n",
    "    for sample in (sample_indices):\n",
    "      #j = randint(0, train_data.shape[1] -1)          #true stochastic descent\n",
    "\n",
    "\n",
    "      current_x = x_train[sample].reshape(1, -1)\n",
    "\n",
    "\n",
    "      y_h, z_h, y_o, z_o = forwardprop(current_x, weights_h, bias_h, w_o, bias_o)\n",
    "      d_weights_h, d_bias_h, d_w_o, d_bias_o = backprop(y_h, z_h, y_o, z_o, weights_h, w_o, current_x, y_train_OH[sample])\n",
    "      weights_h, bias_h, w_o, bias_o = update(weights_h, bias_h, w_o, bias_o, d_weights_h, d_bias_h, d_w_o, d_bias_o)\n",
    "\n",
    "      loss += categorical_cross_entropy(y_train_OH[sample], z_o.squeeze()) / (x_train.shape[0])\n",
    "\n",
    "    loss_list.append(loss)\n",
    "    nb_correct = 0\n",
    "    val_loss = 0\n",
    "    val_idx = np.arange(x_val.shape[0])\n",
    "    for i in val_idx:\n",
    "      y_h, z_h, y_o, z_o = forwardprop(x_val[i], weights_h, bias_h, w_o, bias_o)\n",
    "      val_loss += categorical_cross_entropy(y_val_OH[i], z_o.squeeze()) / (x_val.shape[0])\n",
    "\n",
    "\n",
    "      answer = z_o.argmax()\n",
    "      if answer == y_val[i]:\n",
    "        nb_correct += 1\n",
    "\n",
    "    accuracy = nb_correct / x_val.shape[0]\n",
    "    print(\"validation loss\", val_loss)\n",
    "    print(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "  print(\"train loss\",loss,\"after\",nb_it,\"iterations\")\n",
    "  axe_x = np.arange(len(loss_list))\n",
    "  plt.plot(axe_x, loss_list)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "train(weights_h, bias_h, w_o, bias_o, x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partie test\n",
    "x_display = x_test.reshape(10000, 28, 28)\n",
    "\n",
    "\n",
    "def test(x_test, y_test):\n",
    "\n",
    "  nb_correct = 0\n",
    "  debug = 0\n",
    "  predictions = np.zeros(shape=(y_test.shape[0]))\n",
    "  for i in range(x_test.shape[0]):\n",
    "\n",
    "\n",
    "    _, _, _, z_o = forwardprop(x_test[i], weights_h, bias_h, w_o, bias_o)\n",
    "\n",
    "    predictions[i] = z_o.argmax()\n",
    "\n",
    "\n",
    "    if predictions[i] != y_test[i]: #imprime 10 erreurs\n",
    "      while debug < 10:\n",
    "        print(\"\")\n",
    "        print(\"Attention : erreur\")\n",
    "        print(\"Prediction: \", predictions[i])\n",
    "        print(\"Label: \", y_test[i])\n",
    "        plt.gray()\n",
    "        plt.imshow(x_display[i], interpolation='nearest')\n",
    "        plt.show()\n",
    "        debug += 1\n",
    "        break\n",
    "\n",
    "\n",
    "  accuracy = np.mean(predictions == y_test)\n",
    "  print(\"\")\n",
    "  print(\"Accuracy\", accuracy)\n",
    "  return predictions\n",
    "\n",
    "predictions = test(x_test, y_test)\n",
    "\n",
    "\n",
    "#imprime les données test avec la prediction et le label\n",
    "for i in range(100):\n",
    "\n",
    "  print(\"\")\n",
    "  print(\"Prediction: \", predictions[i])\n",
    "  print(\"Label: \", y_test[i])\n",
    "  plt.gray()\n",
    "  plt.imshow(x_display[i], interpolation='nearest')\n",
    "  plt.show()\n",
    "\n",
    "def validate():\n",
    "  train(weights_h, bias_h, w_o, bias_o, x_train, y_train)\n",
    "\n",
    "  for validations in range(20):\n",
    "    test()\n",
    "\n",
    "\n",
    "validate()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
